{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be5d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# DEPENDENCIES\n",
    "###############################\n",
    "\n",
    "# Import and test pyvirtualdisplay\n",
    "!pip3 install pyvirtualdisplay\n",
    "!export DISPLAY=:0\n",
    "import os\n",
    "os.environ['PYVIRTUALDISPLAY_DISPLAYFD'] = '0'\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d41e47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install gym==0.22\n",
    "!pip install imageio-ffmpeg\n",
    "!pip install huggingface_hub\n",
    "!pip install gymnasium[box2d]==0.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb683638",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# HUGGING FACE METHODS\n",
    "###############################\n",
    "from huggingface_hub import HfApi, upload_folder\n",
    "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
    "\n",
    "def package_to_hub(\n",
    "    repo_id,\n",
    "    model,\n",
    "    hyperparameters,\n",
    "    eval_env,\n",
    "    video_fps=30,\n",
    "    commit_message=\"Push agent to the Hub\",\n",
    "    token=None,\n",
    "    logs=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n",
    "    This method does the complete pipeline:\n",
    "    - It evaluates the model\n",
    "    - It generates the model card\n",
    "    - It generates a replay video of the agent\n",
    "    - It pushes everything to the hub\n",
    "    :param repo_id: id of the model repository from the Hugging Face Hub\n",
    "    :param model: trained model\n",
    "    :param eval_env: environment used to evaluate the agent\n",
    "    :param fps: number of fps for rendering the video\n",
    "    :param commit_message: commit message\n",
    "    :param logs: directory on local machine of tensorboard logs you'd like to upload\n",
    "    \"\"\"\n",
    "    msg.info(\n",
    "        \"This function will save, evaluate, generate a video of your agent, \"\n",
    "        \"create a model card and push everything to the hub. \"\n",
    "        \"It might take up to 1min. \\n \"\n",
    "        \"This is a work in progress: if you encounter a bug, please open an issue.\"\n",
    "    )\n",
    "    # Step 1: Clone or create the repo\n",
    "    repo_url = HfApi().create_repo(\n",
    "        repo_id=repo_id,\n",
    "        token=token,\n",
    "        private=False,\n",
    "        exist_ok=True,\n",
    "    )\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        tmpdirname = Path(tmpdirname)\n",
    "\n",
    "        # Step 2: Save the model\n",
    "        torch.save(model.state_dict(), tmpdirname / \"model.pt\")\n",
    "\n",
    "        # Step 3: Evaluate the model and build JSON\n",
    "        mean_reward, std_reward = _evaluate_agent(eval_env, 10, model)\n",
    "\n",
    "        # First get datetime\n",
    "        eval_datetime = datetime.datetime.now()\n",
    "        eval_form_datetime = eval_datetime.isoformat()\n",
    "\n",
    "        evaluate_data = {\n",
    "            \"env_id\": hyperparameters.env_id,\n",
    "            \"mean_reward\": mean_reward,\n",
    "            \"std_reward\": std_reward,\n",
    "            \"n_evaluation_episodes\": 10,\n",
    "            \"eval_datetime\": eval_form_datetime,\n",
    "        }\n",
    "\n",
    "        # Write a JSON file\n",
    "        with open(tmpdirname / \"results.json\", \"w\") as outfile:\n",
    "            json.dump(evaluate_data, outfile)\n",
    "\n",
    "        # Step 4: Generate a video\n",
    "        video_path = tmpdirname / \"replay.mp4\"\n",
    "        record_video(eval_env, model, video_path, video_fps)\n",
    "\n",
    "        # Step 5: Generate the model card\n",
    "        generated_model_card, metadata = _generate_model_card(\n",
    "            \"PPO\", hyperparameters.env_id, mean_reward, std_reward, hyperparameters\n",
    "        )\n",
    "        _save_model_card(tmpdirname, generated_model_card, metadata)\n",
    "\n",
    "        # Step 6: Add logs if needed\n",
    "        if logs:\n",
    "            _add_logdir(tmpdirname, Path(logs))\n",
    "\n",
    "        msg.info(f\"Pushing repo {repo_id} to the Hugging Face Hub\")\n",
    "\n",
    "        repo_url = upload_folder(\n",
    "            repo_id=repo_id,\n",
    "            folder_path=tmpdirname,\n",
    "            path_in_repo=\"\",\n",
    "            commit_message=commit_message,\n",
    "            token=token,\n",
    "        )\n",
    "\n",
    "        msg.info(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")\n",
    "    return repo_url\n",
    "\n",
    "\n",
    "def _evaluate_agent(env, n_eval_episodes, policy):\n",
    "    \"\"\"\n",
    "    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "    :param env: The evaluation environment\n",
    "    :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "    :param policy: The agent\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    for episode in range(n_eval_episodes):\n",
    "        state = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        while done is False:\n",
    "            state = torch.Tensor(state).to(device)\n",
    "            action, _, _, _ = policy.get_action_and_value(state)\n",
    "            new_state, reward, done, info = env.step(action.cpu().numpy())\n",
    "            total_rewards_ep += reward\n",
    "            if done:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "\n",
    "def record_video(env, policy, out_directory, fps=30):\n",
    "    images = []\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    images.append(img)\n",
    "    while not done:\n",
    "        state = torch.Tensor(state).to(device)\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action, _, _, _ = policy.get_action_and_value(state)\n",
    "        state, reward, done, info = env.step(\n",
    "            action.cpu().numpy()\n",
    "        )  # We directly put next_state = state for recording logic\n",
    "        img = env.render(mode=\"rgb_array\")\n",
    "        images.append(img)\n",
    "    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n",
    "\n",
    "\n",
    "def _generate_model_card(model_name, env_id, mean_reward, std_reward, hyperparameters):\n",
    "    \"\"\"\n",
    "    Generate the model card for the Hub\n",
    "    :param model_name: name of the model\n",
    "    :env_id: name of the environment\n",
    "    :mean_reward: mean reward of the agent\n",
    "    :std_reward: standard deviation of the mean reward of the agent\n",
    "    :hyperparameters: training arguments\n",
    "    \"\"\"\n",
    "    # Step 1: Select the tags\n",
    "    metadata = generate_metadata(model_name, env_id, mean_reward, std_reward)\n",
    "\n",
    "    # Transform the hyperparams namespace to string\n",
    "    converted_dict = vars(hyperparameters)\n",
    "    converted_str = str(converted_dict)\n",
    "    converted_str = converted_str.split(\", \")\n",
    "    converted_str = \"\\n\".join(converted_str)\n",
    "\n",
    "    # Step 2: Generate the model card\n",
    "    model_card = f\"\"\"\n",
    "  # PPO Agent Playing {env_id}\n",
    "\n",
    "  This is a trained model of a PPO agent playing {env_id}.\n",
    "\n",
    "  # Hyperparameters\n",
    "  \"\"\"\n",
    "    return model_card, metadata\n",
    "\n",
    "\n",
    "def generate_metadata(model_name, env_id, mean_reward, std_reward):\n",
    "    \"\"\"\n",
    "    Define the tags for the model card\n",
    "    :param model_name: name of the model\n",
    "    :param env_id: name of the environment\n",
    "    :mean_reward: mean reward of the agent\n",
    "    :std_reward: standard deviation of the mean reward of the agent\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    metadata[\"tags\"] = [\n",
    "        env_id,\n",
    "        \"ppo\",\n",
    "        \"deep-reinforcement-learning\",\n",
    "        \"reinforcement-learning\",\n",
    "        \"custom-implementation\",\n",
    "        \"deep-rl-course\",\n",
    "    ]\n",
    "\n",
    "    # Add metrics\n",
    "    eval = metadata_eval_result(\n",
    "        model_pretty_name=model_name,\n",
    "        task_pretty_name=\"reinforcement-learning\",\n",
    "        task_id=\"reinforcement-learning\",\n",
    "        metrics_pretty_name=\"mean_reward\",\n",
    "        metrics_id=\"mean_reward\",\n",
    "        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n",
    "        dataset_pretty_name=env_id,\n",
    "        dataset_id=env_id,\n",
    "    )\n",
    "\n",
    "    # Merges both dictionaries\n",
    "    metadata = {**metadata, **eval}\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def _save_model_card(local_path, generated_model_card, metadata):\n",
    "    \"\"\"Saves a model card for the repository.\n",
    "    :param local_path: repository directory\n",
    "    :param generated_model_card: model card generated by _generate_model_card()\n",
    "    :param metadata: metadata\n",
    "    \"\"\"\n",
    "    readme_path = local_path / \"README.md\"\n",
    "    readme = \"\"\n",
    "    if readme_path.exists():\n",
    "        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n",
    "            readme = f.read()\n",
    "    else:\n",
    "        readme = generated_model_card\n",
    "\n",
    "    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    # Save our metrics to Readme metadata\n",
    "    metadata_save(readme_path, metadata)\n",
    "\n",
    "\n",
    "def _add_logdir(local_path: Path, logdir: Path):\n",
    "    \"\"\"Adds a logdir to the repository.\n",
    "    :param local_path: repository directory\n",
    "    :param logdir: logdir directory\n",
    "    \"\"\"\n",
    "    if logdir.exists() and logdir.is_dir():\n",
    "        # Add the logdir to the repository under new dir called logs\n",
    "        repo_logdir = local_path / \"logs\"\n",
    "\n",
    "        # Delete current logs if they exist\n",
    "        if repo_logdir.exists():\n",
    "            shutil.rmtree(repo_logdir)\n",
    "\n",
    "        # Copy logdir into repo logdir\n",
    "        shutil.copytree(logdir, repo_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d304e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
